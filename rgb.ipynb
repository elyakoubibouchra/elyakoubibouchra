{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fd1d1a-51a6-4101-89e6-aad85d473997",
      "metadata": {
        "id": "d7fd1d1a-51a6-4101-89e6-aad85d473997",
        "outputId": "d160fd9e-8e67-4c62-8c0f-4a8abca09f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data: (25740, 3072) (25740, 33)\n",
            "Classes: ['ya', 'yab', 'yach', 'yad', 'yadd', 'yae', 'yaf', 'yag', 'yagh', 'yagw', 'yah', 'yahh', 'yaj', 'yak', 'yakw', 'yal', 'yam', 'yan', 'yaq', 'yar', 'yarr', 'yas', 'yass', 'yat', 'yatt', 'yaw', 'yax', 'yay', 'yaz', 'yazz', 'yey', 'yi', 'yu']\n",
            "Exemple X[0] min=-4.451204776763916, max=0.7390037775039673, shape=(3072,)\n",
            "Label Y[0] one-hot: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Classe Y[0] inverse transform: ['ya']\n",
            "=== Test forward + backward sur un batch ===\n",
            "Loss batch: 4.3121\n",
            "Backward pass réussie sur batch\n",
            "Epoch 10: Avg Batch Loss=0.0473, Train Loss=0.0292, Train Acc=99.11%\n",
            "Validation Loss=0.3911, Validation Acc=93.49%\n",
            "Epoch 20: Avg Batch Loss=0.0373, Train Loss=0.0154, Train Acc=99.59%\n",
            "Validation Loss=0.4635, Validation Acc=93.82%\n",
            "Learning rate decayed to 0.000700\n",
            "Epoch 30: Avg Batch Loss=0.0161, Train Loss=0.0004, Train Acc=100.00%\n",
            "Validation Loss=0.3841, Validation Acc=94.91%\n",
            "Epoch 40: Avg Batch Loss=0.0194, Train Loss=0.0006, Train Acc=100.00%\n",
            "Validation Loss=0.4234, Validation Acc=94.72%\n",
            "Learning rate decayed to 0.000490\n",
            "Epoch 50: Avg Batch Loss=0.0168, Train Loss=0.0004, Train Acc=100.00%\n",
            "Validation Loss=0.4053, Validation Acc=94.74%\n",
            "Epoch 60: Avg Batch Loss=0.0110, Train Loss=0.0009, Train Acc=99.99%\n",
            "Validation Loss=0.3408, Validation Acc=95.03%\n",
            "Learning rate decayed to 0.000343\n",
            "Epoch 70: Avg Batch Loss=0.0102, Train Loss=0.0004, Train Acc=100.00%\n",
            "Validation Loss=0.3492, Validation Acc=94.66%\n",
            "Epoch 80: Avg Batch Loss=0.0088, Train Loss=0.0002, Train Acc=100.00%\n",
            "Validation Loss=0.3252, Validation Acc=94.97%\n",
            "Learning rate decayed to 0.000240\n",
            "Epoch 90: Avg Batch Loss=0.0230, Train Loss=0.0037, Train Acc=99.91%\n",
            "Validation Loss=0.3391, Validation Acc=94.37%\n",
            "Epoch 100: Avg Batch Loss=0.0069, Train Loss=0.0002, Train Acc=100.00%\n",
            "Validation Loss=0.2976, Validation Acc=95.12%\n",
            "Training curves saved to 'training_curves.png'\n",
            "Confusion matrix saved to 'confusion_matrix.png'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import joblib\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "def load_data(data_path, image_size=(32, 32)):\n",
        "    X, y = [], []\n",
        "    classes = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
        "    if not classes:\n",
        "        raise ValueError(f\"No classes found in directory '{data_path}'.\")\n",
        "    for label in classes:\n",
        "        class_dir = os.path.join(data_path, label)\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Can't read image {img_path}, skipping.\")\n",
        "                continue\n",
        "            img = cv2.resize(img, image_size)\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    X = np.array(X).astype(np.float32) / 255.0\n",
        "    # Normalisation centrée (mean=0, std=1) par pixel\n",
        "    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)\n",
        "    y = np.array(y)\n",
        "    lb = LabelBinarizer()\n",
        "    y_encoded = lb.fit_transform(y)\n",
        "    return X.reshape(len(X), -1), y_encoded, lb, classes\n",
        "\n",
        "\n",
        "class MLPClassifier:\n",
        "    def __init__(self, input_dim, hidden1, hidden2, output_dim, lr=0.001, reg_lambda=0.001, batch_size=64):\n",
        "        # He initialization pour ReLU\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(input_dim, hidden1) * np.sqrt(2. / input_dim),\n",
        "            'b1': np.zeros((1, hidden1)),\n",
        "            'W2': np.random.randn(hidden1, hidden2) * np.sqrt(2. / hidden1),\n",
        "            'b2': np.zeros((1, hidden2)),\n",
        "            'W3': np.random.randn(hidden2, output_dim) * np.sqrt(2. / hidden2),\n",
        "            'b3': np.zeros((1, output_dim)),\n",
        "        }\n",
        "        self.lr = lr\n",
        "        self.initial_lr = lr\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.batch_size = batch_size\n",
        "        self.adam_params = {}\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "        self.t = 0\n",
        "        for key in self.params:\n",
        "            self.adam_params['m_' + key] = np.zeros_like(self.params[key])\n",
        "            self.adam_params['v_' + key] = np.zeros_like(self.params[key])\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        W1, b1, W2, b2, W3, b3 = self.params.values()\n",
        "        Z1 = X @ W1 + b1\n",
        "        A1 = self.relu(Z1)\n",
        "        Z2 = A1 @ W2 + b2\n",
        "        A2 = self.relu(Z2)\n",
        "        Z3 = A2 @ W3 + b3\n",
        "        A3 = self.softmax(Z3)\n",
        "        cache = (X, Z1, A1, Z2, A2, Z3, A3)\n",
        "        return A3, cache\n",
        "\n",
        "    def compute_loss(self, Y_hat, Y):\n",
        "        m = Y.shape[0]\n",
        "        loss = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
        "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
        "        reg_loss = self.reg_lambda * (np.sum(W1**2) + np.sum(W2**2) + np.sum(W3**2)) / m\n",
        "        return loss + reg_loss\n",
        "\n",
        "    def adam_update(self, grads):\n",
        "        self.t += 1\n",
        "        for key in self.params:\n",
        "            m = self.adam_params['m_' + key]\n",
        "            v = self.adam_params['v_' + key]\n",
        "            g = grads[key]\n",
        "            m[:] = self.beta1 * m + (1 - self.beta1) * g\n",
        "            v[:] = self.beta2 * v + (1 - self.beta2) * (g ** 2)\n",
        "            m_hat = m / (1 - self.beta1 ** self.t)\n",
        "            v_hat = v / (1 - self.beta2 ** self.t)\n",
        "            self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "    def backward(self, cache, Y):\n",
        "        X, Z1, A1, Z2, A2, Z3, A3 = cache\n",
        "        m = X.shape[0]\n",
        "        grads = {}\n",
        "\n",
        "        dZ3 = (A3 - Y) / m\n",
        "        grads['W3'] = A2.T @ dZ3 + self.reg_lambda * self.params['W3'] / m\n",
        "        grads['b3'] = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "        dA2 = dZ3 @ self.params['W3'].T\n",
        "        dZ2 = dA2 * self.relu_derivative(Z2)\n",
        "        grads['W2'] = A1.T @ dZ2 + self.reg_lambda * self.params['W2'] / m\n",
        "        grads['b2'] = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = dZ2 @ self.params['W2'].T\n",
        "        dZ1 = dA1 * self.relu_derivative(Z1)\n",
        "        grads['W1'] = X.T @ dZ1 + self.reg_lambda * self.params['W1'] / m\n",
        "        grads['b1'] = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        self.adam_update(grads)\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y_hat, _ = self.forward(X)\n",
        "        return np.argmax(Y_hat, axis=1)\n",
        "\n",
        "    def plot_weight_histograms(self, epoch):\n",
        "        plt.figure(figsize=(15, 4))\n",
        "        for i, key in enumerate(['W1', 'W2', 'W3']):\n",
        "            plt.subplot(1, 3, i + 1)\n",
        "            plt.hist(self.params[key].flatten(), bins=50, color='c')\n",
        "            plt.title(f'Histogram of {key} weights - Epoch {epoch}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'weights_hist_epoch_{epoch}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def visualize_predictions(self, X_batch, Y_batch, lb, epoch, n_samples=10):\n",
        "        Y_pred_probs, _ = self.forward(X_batch)\n",
        "        Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
        "        Y_true = np.argmax(Y_batch, axis=1)\n",
        "\n",
        "        fig, axes = plt.subplots(2, n_samples, figsize=(20, 5))\n",
        "        for i in range(n_samples):\n",
        "            img = X_batch[i].reshape(32, 32, 3)\n",
        "            # dé-normaliser pour afficher correctement (approx)\n",
        "            img = (img * np.std(X_batch, axis=0).reshape(32, 32, 3)) + np.mean(X_batch, axis=0).reshape(32, 32, 3)\n",
        "            img = np.clip(img, 0, 1)\n",
        "            axes[0, i].imshow(img)\n",
        "            axes[0, i].axis('off')\n",
        "            axes[0, i].set_title(f\"True: {lb.classes_[Y_true[i]]}\")\n",
        "            axes[1, i].imshow(img)\n",
        "            axes[1, i].axis('off')\n",
        "            axes[1, i].set_title(f\"Pred: {lb.classes_[Y_pred[i]]}\")\n",
        "        plt.suptitle(f\"Predictions at Epoch {epoch}\")\n",
        "        plt.savefig(f'predictions_epoch_{epoch}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def fit(self, X, Y, X_val=None, Y_val=None, epochs=100):\n",
        "        n_samples = X.shape[0]\n",
        "        train_losses, train_accuracies = [], []\n",
        "        val_losses, val_accuracies = [], []\n",
        "\n",
        "        print(\"=== Test forward + backward sur un batch ===\")\n",
        "        X_batch = X[:self.batch_size]\n",
        "        Y_batch = Y[:self.batch_size]\n",
        "        Y_hat, cache = self.forward(X_batch)\n",
        "        loss = self.compute_loss(Y_hat, Y_batch)\n",
        "        print(f\"Loss batch: {loss:.4f}\")\n",
        "        self.backward(cache, Y_batch)\n",
        "        print(\"Backward pass réussie sur batch\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Decay learning rate every 20 epochs\n",
        "            if epoch > 0 and epoch % 20 == 0:\n",
        "                self.lr *= 0.7\n",
        "                print(f\"Learning rate decayed to {self.lr:.6f}\")\n",
        "\n",
        "            permutation = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_shuffled = Y[permutation]\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                Y_batch = Y_shuffled[i:i+self.batch_size]\n",
        "                Y_hat, cache = self.forward(X_batch)\n",
        "                batch_loss = self.compute_loss(Y_hat, Y_batch)\n",
        "                batch_losses.append(batch_loss)\n",
        "                self.backward(cache, Y_batch)\n",
        "\n",
        "            Y_hat_full, _ = self.forward(X)\n",
        "            loss = self.compute_loss(Y_hat_full, Y)\n",
        "            acc = np.mean(np.argmax(Y_hat_full, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "            train_losses.append(loss)\n",
        "            train_accuracies.append(acc)\n",
        "\n",
        "            if X_val is not None and Y_val is not None:\n",
        "                Y_val_hat, _ = self.forward(X_val)\n",
        "                val_loss = self.compute_loss(Y_val_hat, Y_val)\n",
        "                val_acc = np.mean(np.argmax(Y_val_hat, axis=1) == np.argmax(Y_val, axis=1))\n",
        "                val_losses.append(val_loss)\n",
        "                val_accuracies.append(val_acc)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                avg_batch_loss = np.mean(batch_losses)\n",
        "                print(f\"Epoch {epoch+1}: Avg Batch Loss={avg_batch_loss:.4f}, Train Loss={loss:.4f}, Train Acc={acc*100:.2f}%\")\n",
        "                if X_val is not None and Y_val is not None:\n",
        "                    print(f\"Validation Loss={val_loss:.4f}, Validation Acc={val_acc*100:.2f}%\")\n",
        "\n",
        "                # Visualisation histogramme poids\n",
        "                self.plot_weight_histograms(epoch + 1)\n",
        "\n",
        "                # Visualisation prédictions sur un batch (dernier batch du shuffle)\n",
        "                self.visualize_predictions(X_batch, Y_batch, lb, epoch + 1, n_samples=10)\n",
        "\n",
        "        # Courbes globales loss / accuracy\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Train Loss')\n",
        "        if val_losses:\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title(\"Loss curve\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_accuracies, label='Train Accuracy')\n",
        "        if val_accuracies:\n",
        "            plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.title(\"Accuracy curve\")\n",
        "\n",
        "        plt.savefig('training_curves.png')\n",
        "        print(\"Training curves saved to 'training_curves.png'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X, Y, lb, classes = load_data('./dataset/AMHCD_64/AMHCD_64')\n",
        "\n",
        "    print(\"Loaded data:\", X.shape, Y.shape)\n",
        "    print(f\"Classes: {classes}\")\n",
        "    print(f\"Exemple X[0] min={X[0].min()}, max={X[0].max()}, shape={X[0].shape}\")\n",
        "    print(f\"Label Y[0] one-hot: {Y[0]}\")\n",
        "    print(f\"Classe Y[0] inverse transform: {lb.inverse_transform(Y[0].reshape(1, -1))}\")\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "    model = MLPClassifier(input_dim=input_dim, hidden1=64, hidden2=32, output_dim=len(classes), lr=0.001, batch_size=64)\n",
        "    model.fit(X_train, Y_train, X_val, Y_val, epochs=100)\n",
        "\n",
        "    joblib.dump(model, 'mlp_tifinagh_rgb_adam_debug.pkl')\n",
        "    joblib.dump(lb, 'label_binarizer.pkl')\n",
        "\n",
        "    # === Confusion Matrix ===\n",
        "    Y_val_pred = model.predict(X_val)\n",
        "    Y_val_true = np.argmax(Y_val, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(Y_val_true, Y_val_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax)\n",
        "    plt.title(\"Confusion Matrix - Validation Set\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved to 'confusion_matrix.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722c62d7-2f1e-4be4-8f64-a1548010c80e",
      "metadata": {
        "id": "722c62d7-2f1e-4be4-8f64-a1548010c80e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}